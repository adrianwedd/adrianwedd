Testing LLM chat responses presents a unique challenge as the actual LLM interaction occurs within a GitHub Actions workflow, with responses stored as JSON files in the repository. Direct, real-time testing of the LLM's output is difficult without complex mocking or a dedicated test environment.

**Current State:**
- LLM chat responses are generated by a GitHub Actions workflow triggered by `/api/chat`.
- Responses are stored as JSON files in the repository and retrieved by `/api/chat-status`.
- No dedicated tests for LLM response quality or correctness.

**Affected Files:**
- `api/chat.js`
- `api/chat-status.js`
- `.github/workflows/claude-autonomous.yml` (or similar workflow for LLM processing)
- `chat-responses/` (directory for storing LLM responses)

**Proposed Resolution:**
Implement LLM chat response testing with a focus on:
1.  **Offline/Mocked Response Testing:**
    *   Create a suite of tests that use pre-recorded LLM responses (e.g., JSON files) to verify the terminal's parsing and display of these responses.
    *   This would involve mocking the `fetch` calls in `api/chat-status.js` to return these static response files.
2.  **Workflow-Triggered Validation (Manual/Semi-Automated):**
    *   Develop a separate GitHub Actions workflow that can be manually triggered or run on a schedule.
    *   This workflow would:
        *   Send a predefined set of prompts to the LLM.
        *   Capture the LLM's responses.
        *   Perform automated checks on the responses (e.g., keyword presence, length, basic sentiment analysis) to identify major regressions.
        *   Generate a report (e.g., a new issue or a comment on an existing issue) with the results for manual review.
3.  **Focus on Integration Points:** Ensure that the data flow from the terminal -> `api/chat.js` -> GitHub Actions -> `chat-responses` -> `api/chat-status.js` -> terminal is robust and handles various scenarios (e.g., long responses, errors).
4.  **Human-in-the-Loop Review:** For critical LLM responses, incorporate a manual review step where human testers can evaluate the quality and correctness of the AI's output.

**Labels:** `test`, `priority: high`, `type: enhancement`